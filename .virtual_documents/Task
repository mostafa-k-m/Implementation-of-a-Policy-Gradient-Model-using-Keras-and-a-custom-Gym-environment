import pandas as pd
from datetime import datetime, date, time, timedelta
import warnings
warnings.filterwarnings("ignore")

df = pd.read_csv('data.csv').drop('Unnamed: 24', axis=1)
df = df[(df.CANCELLED == 0) & (df.DIVERTED == 0)].drop(
    ['CANCELLED', 'DIVERTED'], axis=1)


df['x1'] = df['TAXI_OUT']


def time_converter(in_):
    in_date = in_[0]
    in_time = in_[1]
    in_time = str(in_time)
    if '.' in in_time:
        in_time = in_time[:-2]
    while len(str(in_time)) < 4:
        in_time = '0' + in_time
    if in_time[:2] == '24':
        in_time = '00' + in_time[2:]
    time_ = time(int(in_time[:2]), int(in_time[2:4]), 0)
    in_date = in_date.split('-')
    date_ = date(int(in_date[0]), int(in_date[1]), int(in_date[2]))
    return datetime.combine(date_, time_)


time_cols = ['CRS_DEP_TIME', 'DEP_TIME', 'WHEELS_OFF',
             'WHEELS_ON', 'CRS_ARR_TIME', 'ARR_TIME', ]

for i, col in enumerate(time_cols):
    df[col] = df[['FL_DATE', col]].apply(time_converter, axis=1)
    print(f"{round(100*(i+1)/len(time_cols),2)}% done.")


def get_bounds(in_time):
    time_d = timedelta(minutes=20)
    return (in_time - time_d),  (in_time + time_d)


def window_func(TIME, variable, df):
    lower_bound, upper_bound = get_bounds(TIME)
    df_ = df[(df[variable] >= lower_bound) & (df[variable] <=
                                              upper_bound)].copy()
    count_airplanes = len(df_) - 1
    return count_airplanes


def pct_tracker(to_be_done):
    done = 0
    count = 0
    while True:
        yield
        done += 1
        count += 1
        if done > to_be_done:
            done -= to_be_done
        if (count >= to_be_done * 0.02):
            count = 0
            pct = round((float(done) / float(to_be_done)) * 100, 2)
            print(f"{pct}% done.")


def track_progress(func=window_func, progress=pct_tracker(len(df))):
    def call_func(*args, **kwargs):
        progress.send(None)
        return func(*args, **kwargs)
    return call_func


def airplane_counter(func, variable, airport_type, list, add_cols=[]):
    listy = []
    sub_df = df.sort_values(by=[airport_type, variable])[
        [airport_type, variable] + add_cols].copy()
    for i, airport in enumerate(list):
        x = sub_df[sub_df[airport_type] == airport].copy()
        x['target'] = x[variable].apply(
            track_progress(func), args=([variable, x]))
        listy.append(x)
    series = pd.concat(listy).sort_values(by=[airport_type, variable]).target
    return series


df = df.sort_values(by=['ORIGIN', 'DEP_TIME']).copy()
origins = df.ORIGIN.unique().tolist()

x2 = airplane_counter(window_func, 'DEP_TIME', 'ORIGIN', origins)


df['x2'] = x2

df = df.sort_values(by=['DEST', 'ARR_TIME']).copy()
destinations = df.DEST.unique().tolist()

sub_df = df.sort_values(by=['DEST', 'ARR_TIME'])[
    ['DEST', 'ARR_TIME', 'ARR_DELAY']].copy()
dict_ = {dest: sub_df[sub_df['DEST'] == dest].copy() for dest in destinations}


def window_func_arr(in_):
    ORIGIN, TIME = in_[0], in_[1]
    lower_bound, upper_bound = get_bounds(TIME)
    df = dict_[ORIGIN]
    df_ = df[(df['ARR_TIME'] >= lower_bound) & (df['ARR_TIME'] <=
                                                upper_bound)].copy()
    count_airplanes = len(df_)
    return count_airplanes


df['x3'] = df[['ORIGIN', 'DEP_TIME']].apply(
    track_progress(window_func_arr), axis=1)


def is_peak(dep_time):
    dep_time = dep_time.time()
    if (dep_time >= time(4, 0) and (dep_time <= time(16, 0))):
        return 1
    else:
        return 2


dict_averages={origin: df[df.ORIGIN == origin]
                 ['TAXI_OUT'].mean() for origin in origins}


def avg_taxi_out(origin, dict=dict_averages):
    return dict[origin]


df['x4']=df.ORIGIN.apply(avg_taxi_out)

df['x5']=df.DEP_TIME.apply(is_peak)

df['x6']=df.AIR_TIME

df['x7']=df.TAXI_IN

df.to_csv('snapshot.csv', index=False)


def late_to_leave(TIME, variable, df):
    lower_bound, upper_bound=get_bounds(TIME)
    df_=df[(df[variable] >= lower_bound) & (df[variable] <=
                                              upper_bound) & ((df['DEP_DELAY'] < -15) | (df['DEP_DELAY'] > 15))].copy()
    count_airplanes=len(df_)
    return count_airplanes

x8=airplane_counter(late_to_leave, 'DEP_TIME', 'ORIGIN',
                    origins, add_cols=['DEP_DELAY'])

df['x8']=x8


def late_to_arrive(in_):
    ORIGIN, TIME=in_[0], in_[1]
    lower_bound, upper_bound=get_bounds(TIME)
    df=dict_[ORIGIN]
    df_=df[(df['ARR_TIME'] >= lower_bound) & (df['ARR_TIME'] <=
                                                upper_bound) & ((df['ARR_DELAY'] < -15) | (df['ARR_DELAY'] > 15))].copy()
    count_airplanes=len(df_)
    return count_airplanes

df['x9']=df[['ORIGIN', 'DEP_TIME']].apply(
    track_progress(late_to_arrive), axis=1)

df[['ORIGIN', 'DEP_TIME'] +
    ['x' + str(i) for i in range(1, 10)]].to_csv('variables.csv', index=False)

work_df=df[['ORIGIN', 'DEP_TIME'] +
    ['x' + str(i) for i in range(1, 10)]]


work_df['DEP_TIME']=pd.to_datetime(work_df['DEP_TIME'])
work_df=pd.read_csv('./env/data/variables.csv')
print('non discretized environment variables')
work_df.iloc[:10,:]


import random
import gym
from gym import spaces
import numpy as np
from datetime import datetime, timedelta

class ontime_dataset_env(gym.Env):
    metadata = {'render.modes': ['human']}

    def __init__(self, df):
        super(ontime_dataset_env, self).__init__()
        self.df = df
        # getting the maximum of each variable because variables have to be bound between 0 and 1
        self.max_x2 = max(df['x2'])
        self.max_x3 = max(df['x3'])
        self.max_x4 = max(df['x4'])
        self.max_x5 = max(df['x5'])
        self.max_x6 = max(df['x6'])
        self.max_x7 = max(df['x7'])
        self.max_x8 = max(df['x8'])
        self.max_x9 = max(df['x9'])
        self.prev_time_step = datetime.strptime(
            '2019-03-30 23:45:01', 'get_ipython().run_line_magic("Y-%m-%d", " %H:%M:%S')")
        n_actions = 5
        #defining action space, we are going to discritize our actions into 5 bins
        self.action_space = spaces.Discrete(n_actions)
        
        #defining observation space
        self.observation_space = spaces.Box(
            low=0, high=1, shape=(8,), dtype=np.float16)

    def _next_observation(self):
        # construct the state (observation) for every step, the state will include all the airplains in a window of 15 minutes and we will pad the resulting array with zeros to reach a uniform shape
        obs = np.array([
            self.df.loc[self.current_step: self.current_step
                        + timedelta(minutes=15), 'x1'].values,
            self.df.loc[self.current_step: self.current_step
                        + timedelta(minutes=15), 'x2'].values / self.max_x2,
            self.df.loc[self.current_step: self.current_step
                        + timedelta(minutes=15), 'x3'].values / self.max_x3,
            self.df.loc[self.current_step: self.current_step
                        + timedelta(minutes=15), 'x4'].values / self.max_x4,
            self.df.loc[self.current_step: self.current_step
                        + timedelta(minutes=15), 'x5'].values / self.max_x5,
            self.df.loc[self.current_step: self.current_step
                        + timedelta(minutes=15), 'x6'].values / self.max_x6,
            self.df.loc[self.current_step: self.current_step
                        + timedelta(minutes=15), 'x7'].values / self.max_x7,
            self.df.loc[self.current_step: self.current_step
                        + timedelta(minutes=15), 'x8'].values / self.max_x8,
            self.df.loc[self.current_step: self.current_step
                        + timedelta(minutes=15), 'x9'].values / self.max_x9
        ])
        #fetch next row until the window is done then get next window
        if self.prev_time_step get_ipython().getoutput("= self.current_step:")
            self.row_idx = 0
            self.input_length = obs.shape[1]
        obs = obs[:, self.row_idx]
        self.prev_time_step = self.current_step
        return obs
    
    def _take_action(self, action):
        # choose a random action
        current_dep_latency = random.randint(0, 4)

    def step(self, action):
        # Execute one time step within the environment
        self._take_action(action)
        obs = self._next_observation()        
        #if we get to the end of the dataset while taking a step, we return to the begining

        
        #calculating the reward, it will be one for each correct lable and -1* abs(true_label - predicted_label)/4 for each incorrect label
        reward = np.absolute(obs[0] - action)*-1/4
        if reward == 0:
            reward = 1
        self.row_idx += 1
        
        # we will end each episode when we have trained on each row within a 15 minutes window, also if we randomly chose a starting ppoint in the last 15 minutes of the dataset we will be reset to the begining
        done = False if self.row_idx < self.input_length else True
        if done:
            self.current_step = self.current_step = self.df.iloc[random.randint(0, len(self.df.loc[:, 'x2'].values) - 6)].name 
            if self.current_step > datetime.strptime('2019-04-30 23:45:01', 'get_ipython().run_line_magic("Y-%m-%d", " %H:%M:%S'):")
                self.current_step = datetime.strptime(
                    '2019-04-01 00:00:00', 'get_ipython().run_line_magic("Y-%m-%d", " %H:%M:%S')")
        return obs, reward, done, {}

    def reset(self):
        # Set the current step to a random point within the data frame
        self.current_step = self.df.iloc[random.randint(
            0, len(self.df.loc[:, 'x2'].values) - 6)].name
        return self._next_observation()


    def render(self, mode='human', close=False):
        pass


import tensorflow as tf
gpus = tf.config.experimental.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(gpus[0], True)
from tensorflow import keras
from keras.layers import Dense, Activation, Input, Flatten
from keras.models import Model, load_model
from keras.optimizers import Adam
import keras.backend as K
import numpy as np


class Agent(object):
    def __init__(self, ALPHA, GAMMA=0.99, n_actions=5, layer1_size=16, layer2_size=16, layer3_size=16, input_dims=9, fname='model.h5'):
        self.GAMMA = GAMMA
        self.lr = ALPHA
        self.G = 0
        self.input_dims = input_dims
        self.fc1_dims = layer1_size
        self.fc2_dims = layer2_size
        self.fc3_dims = layer3_size
        self.n_actions = n_actions
        self.state_memory = []
        self.action_memory = []
        self.reward_memory = []
        self.policy, self.predict = self.build_policy_network()
        self.action_space = [i for i in range(n_actions)]
        self.model_file = fname
    
    #this is the function that will define the policy agent and and will make predictions
    def build_policy_network(self):
        input = Input(shape=(self.input_dims,))
        advantages = Input(shape=[1])
        dense1 = Dense(self.fc1_dims, activation='relu')(input)
        dense2 = Dense(self.fc2_dims, activation='relu')(dense1)
        dense3 = Dense(self.fc3_dims, activation='relu')(dense2)
        probs = Dense(self.n_actions, activation='softmax')(dense3)

        def custom_loss(y_true, y_predict):
            out = K.clip(y_predict, 1e-8, 1 - 1e-8)
            log_lik = y_true * K.log(out)
            return K.sum(-log_lik * advantages)

        policy = Model(input=[input, advantages], output=[probs])
        policy.compile(optimizer=Adam(lr=self.lr), loss=custom_loss)
        predict = Model(input=[input], output=[probs])
        return policy, predict
    
    #we will choose an actiom for each observation within the batch by making a random choice weighted by the probabilities predicted by our model
    def choose_action(self, observation):
        state = observation[np.newaxis, :]
        probabilities = self.predict.predict(state)[0]
        action = np.random.choice(self.action_space, p=probabilities)
        return action

    #this is a helper function that stores the history of the model
    def store_transition(self, observation, action, reward):
        self.state_memory.append(observation)
        self.action_memory.append(action)
        self.reward_memory.append(reward)
        
    # this is the main driver function that we will call to train the agent
    def learn(self):
        state_memory = np.array(self.state_memory)
        action_memory = np.array(self.action_memory)
        reward_memory = np.array(self.reward_memory)
        actions = np.zeros([len(action_memory), self.n_actions])
        actions[np.arange(len(action_memory)), action_memory] = 1
        
        # calculating the gain
        G = np.zeros_like(reward_memory)
        for t in range(len(reward_memory)):
            G_sum = 0
            discount = 1
            for k in range(t, len(reward_memory)):
                G_sum += reward_memory[k]*discount
                discount *= self.GAMMA
            G[t] = G_sum
        mean = np.mean(G)
        std = np.std(G) if np.std(G) > 0 else 1
        self.G = (G-mean)/std
        
        #calculating the cost
        cost = self.policy.train_on_batch([state_memory, self.G], actions)
        
        #resetting memory
        self.state_memory = []
        self.action_memory = []
        self.reward_memory = []

    
    #helper function to save and load the model
    def save_model(self):
        self.policy.save(self.model_file)

    def load_model(self):
        self.policy = load_model(self.model_file)


import gym
import pandas as pd
import numpy as np
from sklearn.preprocessing import KBinsDiscretizer

df = pd.read_csv('./env/data/variables.csv')
df['Date'] = pd.to_datetime(df['Date'])
df = df.set_index('Date')
discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal')
df['x1'] = discretizer.fit_transform(
df['x1'].to_numpy().reshape(-1, 1))

env = ontime_dataset_env(df)


env.reset()


env.step(4)


env.step(4)


agent = Agent(ALPHA=0.001, GAMMA=0.99, n_actions=5,
                  layer1_size=32, layer2_size=16,
                  layer3_size=8, fname='model.h5')
env = ontime_dataset_env(df)
score_history = []

n_episodes = 2500

for i in range(n_episodes):
    done = False
    score = 0
    observation = env.reset()
    while not done:
        action = agent.choose_action(observation)
        observation_, reward, done, info = env.step(action)
        agent.store_transition(observation, action, reward)
        observation = observation_
        score += reward
    score_history.append(score)
    agent.learn()
    print('episode', i, 'score get_ipython().run_line_magic(".1f'", " % score, 'average_score %.1f' %")
         np.mean(score_history[-100:]))

agent.save_model()


from gym_model.Agent import GymAgent
from gym_model.gym_env import run_model


run_model()
